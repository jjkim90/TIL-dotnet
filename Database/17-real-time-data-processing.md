# ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬

## ê°œìš”

ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ëŠ” ë°ì´í„°ê°€ ìƒì„±ë˜ëŠ” ì¦‰ì‹œ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•˜ëŠ” ê¸°ìˆ ë¡œ, ê¸ˆìœµ ê±°ë˜, IoT ì„¼ì„œ ë°ì´í„°, ì†Œì…œ ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼, ë¡œê·¸ ë¶„ì„ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì´ ì¥ì—ì„œëŠ” ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ ì•„í‚¤í…ì²˜, ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒ, ê·¸ë¦¬ê³  ì‹¤ì‹œê°„ ë¶„ì„ ì‹œìŠ¤í…œ êµ¬í˜„ ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

## 1. ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ê¸°ì´ˆ

### 1.1 Apache Kafka ê¸°ë°˜ ì•„í‚¤í…ì²˜

```python
from kafka import KafkaProducer, KafkaConsumer
from kafka.admin import KafkaAdminClient, NewTopic
import json
import time
from datetime import datetime
import threading
import avro.schema
import avro.io
import io

class KafkaStreamingPlatform:
    """Kafka ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼"""
    
    def __init__(self, bootstrap_servers='localhost:9092'):
        self.bootstrap_servers = bootstrap_servers
        self.admin_client = KafkaAdminClient(
            bootstrap_servers=bootstrap_servers,
            client_id='streaming_platform'
        )
        
    def create_topics_with_configuration(self):
        """ìµœì í™”ëœ í† í”½ ìƒì„±"""
        topics = [
            NewTopic(
                name='transactions',
                num_partitions=12,
                replication_factor=3,
                topic_configs={
                    'retention.ms': '86400000',  # 1ì¼
                    'segment.ms': '3600000',     # 1ì‹œê°„
                    'cleanup.policy': 'delete',
                    'compression.type': 'lz4',
                    'min.insync.replicas': '2'
                }
            ),
            NewTopic(
                name='user-events',
                num_partitions=24,
                replication_factor=3,
                topic_configs={
                    'retention.ms': '604800000',  # 7ì¼
                    'compression.type': 'snappy',
                    'max.message.bytes': '1048576'  # 1MB
                }
            ),
            NewTopic(
                name='metrics',
                num_partitions=6,
                replication_factor=2,
                topic_configs={
                    'retention.ms': '21600000',   # 6ì‹œê°„
                    'cleanup.policy': 'compact',
                    'segment.ms': '600000',       # 10ë¶„
                    'min.cleanable.dirty.ratio': '0.1'
                }
            )
        ]
        
        try:
            self.admin_client.create_topics(topics)
            print("í† í”½ ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"í† í”½ ìƒì„± ì˜¤ë¥˜: {e}")
    
    def implement_exactly_once_producer(self):
        """Exactly-Once ì‹œë§¨í‹± í”„ë¡œë“€ì„œ"""
        producer = KafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            key_serializer=lambda k: k.encode('utf-8'),
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            acks='all',  # ëª¨ë“  ë³µì œë³¸ í™•ì¸
            enable_idempotence=True,  # ë©±ë“±ì„± í™œì„±í™”
            max_in_flight_requests_per_connection=5,
            retries=10,
            retry_backoff_ms=100,
            transactional_id='transaction-producer-1'  # íŠ¸ëœì­ì…˜ ID
        )
        
        # íŠ¸ëœì­ì…˜ ì´ˆê¸°í™”
        producer.init_transactions()
        
        try:
            # íŠ¸ëœì­ì…˜ ì‹œì‘
            producer.begin_transaction()
            
            # ì—¬ëŸ¬ ë©”ì‹œì§€ë¥¼ ì›ìì ìœ¼ë¡œ ì „ì†¡
            for i in range(100):
                transaction = {
                    'transaction_id': f'TXN-{datetime.now().timestamp()}-{i}',
                    'user_id': f'USER-{i % 10}',
                    'amount': 100.0 + i,
                    'timestamp': datetime.now().isoformat(),
                    'type': 'PURCHASE'
                }
                
                producer.send(
                    'transactions',
                    key=transaction['transaction_id'],
                    value=transaction
                )
            
            # íŠ¸ëœì­ì…˜ ì»¤ë°‹
            producer.commit_transaction()
            print("íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì™„ë£Œ")
            
        except Exception as e:
            producer.abort_transaction()
            print(f"íŠ¸ëœì­ì…˜ ì‹¤íŒ¨: {e}")
        finally:
            producer.close()
    
    def create_stream_processor(self):
        """ìŠ¤íŠ¸ë¦¼ í”„ë¡œì„¸ì„œ êµ¬í˜„"""
        from kafka import TopicPartition
        
        consumer = KafkaConsumer(
            'transactions',
            bootstrap_servers=self.bootstrap_servers,
            group_id='stream-processor',
            enable_auto_commit=False,  # ìˆ˜ë™ ì»¤ë°‹
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            max_poll_records=500,
            session_timeout_ms=30000,
            heartbeat_interval_ms=10000
        )
        
        # ìƒíƒœ ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” RocksDB ë“± ì‚¬ìš©)
        state_store = {}
        
        def process_batch(records):
            """ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§"""
            aggregations = {}
            
            for record in records:
                user_id = record.value['user_id']
                amount = record.value['amount']
                
                # ì‚¬ìš©ìë³„ ì§‘ê³„
                if user_id not in aggregations:
                    aggregations[user_id] = {
                        'count': 0,
                        'total_amount': 0,
                        'last_updated': None
                    }
                
                aggregations[user_id]['count'] += 1
                aggregations[user_id]['total_amount'] += amount
                aggregations[user_id]['last_updated'] = record.value['timestamp']
            
            # ìƒíƒœ ì €ì¥ì†Œ ì—…ë°ì´íŠ¸
            for user_id, stats in aggregations.items():
                if user_id not in state_store:
                    state_store[user_id] = stats
                else:
                    state_store[user_id]['count'] += stats['count']
                    state_store[user_id]['total_amount'] += stats['total_amount']
                    state_store[user_id]['last_updated'] = stats['last_updated']
            
            return aggregations
        
        # ì²˜ë¦¬ ë£¨í”„
        try:
            while True:
                records = consumer.poll(timeout_ms=1000)
                
                if records:
                    # íŒŒí‹°ì…˜ë³„ ì²˜ë¦¬
                    for partition, partition_records in records.items():
                        results = process_batch(partition_records)
                        print(f"íŒŒí‹°ì…˜ {partition.partition} ì²˜ë¦¬: {len(partition_records)} ë ˆì½”ë“œ")
                    
                    # ì˜¤í”„ì…‹ ì»¤ë°‹
                    consumer.commit()
                    
        except KeyboardInterrupt:
            print("í”„ë¡œì„¸ì„œ ì¢…ë£Œ")
        finally:
            consumer.close()
```

### 1.2 Apache Flink ì‹¤ì‹œê°„ ì²˜ë¦¬

```python
# Flink Python API (PyFlink) ì˜ˆì œ
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings
from pyflink.table.udf import udf
from pyflink.table.expressions import col, lit
import pandas as pd

class FlinkStreamProcessor:
    """Apache Flink ìŠ¤íŠ¸ë¦¼ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self):
        # ì‹¤í–‰ í™˜ê²½ ì„¤ì •
        env_settings = EnvironmentSettings.new_instance()\
            .in_streaming_mode()\
            .use_blink_planner()\
            .build()
            
        self.env = StreamExecutionEnvironment.get_execution_environment()
        self.t_env = StreamTableEnvironment.create(self.env, env_settings)
        
        # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
        self.env.enable_checkpointing(10000)  # 10ì´ˆë§ˆë‹¤
        
    def create_kafka_source_table(self):
        """Kafka ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„±"""
        kafka_source_ddl = """
        CREATE TABLE transactions (
            transaction_id STRING,
            user_id STRING,
            amount DOUBLE,
            transaction_time TIMESTAMP(3),
            category STRING,
            WATERMARK FOR transaction_time AS transaction_time - INTERVAL '5' SECOND
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'transactions',
            'properties.bootstrap.servers' = 'localhost:9092',
            'properties.group.id' = 'flink-consumer',
            'scan.startup.mode' = 'latest-offset',
            'format' = 'json',
            'json.fail-on-missing-field' = 'false',
            'json.ignore-parse-errors' = 'true'
        )
        """
        
        self.t_env.execute_sql(kafka_source_ddl)
        
    def implement_windowed_aggregation(self):
        """ìœˆë„ìš° ì§‘ê³„ êµ¬í˜„"""
        # ì„¸ì…˜ ìœˆë„ìš° ì§‘ê³„
        session_window_query = """
        SELECT 
            user_id,
            SESSION_START(transaction_time, INTERVAL '30' MINUTE) as session_start,
            SESSION_END(transaction_time, INTERVAL '30' MINUTE) as session_end,
            COUNT(*) as transaction_count,
            SUM(amount) as total_amount,
            AVG(amount) as avg_amount,
            MAX(amount) as max_amount,
            COLLECT(DISTINCT category) as categories
        FROM transactions
        GROUP BY 
            user_id,
            SESSION(transaction_time, INTERVAL '30' MINUTE)
        """
        
        # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì§‘ê³„
        sliding_window_query = """
        SELECT 
            user_id,
            HOP_START(transaction_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE) as window_start,
            HOP_END(transaction_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE) as window_end,
            COUNT(*) as transaction_count,
            SUM(amount) as total_amount,
            -- ì´ë™ í‰ê· 
            SUM(amount) / COUNT(*) as moving_average
        FROM transactions
        GROUP BY 
            user_id,
            HOP(transaction_time, INTERVAL '1' MINUTE, INTERVAL '5' MINUTE)
        """
        
        # í…€ë¸”ë§ ìœˆë„ìš°ì™€ ì¡°ê¸° ë°œìƒ
        tumbling_window_query = """
        SELECT 
            TUMBLE_START(transaction_time, INTERVAL '1' HOUR) as hour_start,
            category,
            COUNT(*) as count,
            SUM(amount) as revenue,
            -- ì§€ì—° ë°ì´í„° ì²˜ë¦¬
            COUNT(*) FILTER (WHERE transaction_time < CURRENT_WATERMARK()) as on_time_count
        FROM transactions
        GROUP BY 
            TUMBLE(transaction_time, INTERVAL '1' HOUR),
            category
        EMIT 
            -- ì¡°ê¸° ê²°ê³¼ ë°œìƒ
            WITH DELAY '10' SECOND,
            -- ìµœì¢… ê²°ê³¼
            WITHOUT DELAY AFTER WATERMARK
        """
        
        return session_window_query, sliding_window_query, tumbling_window_query
    
    def implement_pattern_detection(self):
        """ë³µì¡í•œ ì´ë²¤íŠ¸ íŒ¨í„´ ê°ì§€ (CEP)"""
        fraud_detection_query = """
        SELECT *
        FROM transactions
        MATCH_RECOGNIZE (
            PARTITION BY user_id
            ORDER BY transaction_time
            MEASURES
                FIRST(A.transaction_id) as first_transaction,
                LAST(D.transaction_id) as last_transaction,
                SUM(A.amount + B.amount + C.amount + D.amount) as total_amount
            ONE ROW PER MATCH
            AFTER MATCH SKIP TO NEXT ROW
            PATTERN (A B C D) WITHIN INTERVAL '10' MINUTE
            DEFINE
                A AS A.amount > 1000,
                B AS B.amount > A.amount * 1.5,
                C AS C.amount > B.amount * 1.5,
                D AS D.amount > C.amount * 1.5
        ) AS suspected_fraud
        """
        
        # ì´ìƒ íƒì§€ íŒ¨í„´
        anomaly_detection_query = """
        SELECT 
            user_id,
            transaction_time,
            amount,
            -- ì´ë™ í‘œì¤€í¸ì°¨ ê³„ì‚°
            STDDEV(amount) OVER (
                PARTITION BY user_id 
                ORDER BY transaction_time 
                RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW
            ) as moving_stddev,
            -- ì´ìƒì¹˜ ìŠ¤ì½”ì–´
            ABS(amount - AVG(amount) OVER (
                PARTITION BY user_id 
                ORDER BY transaction_time 
                RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW
            )) / NULLIF(STDDEV(amount) OVER (
                PARTITION BY user_id 
                ORDER BY transaction_time 
                RANGE BETWEEN INTERVAL '1' HOUR PRECEDING AND CURRENT ROW
            ), 0) as z_score
        FROM transactions
        """
        
        return fraud_detection_query, anomaly_detection_query
```

## 2. ìŠ¤íŠ¸ë¦¼-í…Œì´ë¸” ì¡°ì¸

### 2.1 ì‹¤ì‹œê°„ ë°ì´í„° ë³´ê°•

```python
class StreamTableJoinProcessor:
    """ìŠ¤íŠ¸ë¦¼-í…Œì´ë¸” ì¡°ì¸ ì²˜ë¦¬"""
    
    def __init__(self):
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        self.postgres_pool = psycopg2.pool.SimpleConnectionPool(1, 20, 
            host='localhost', database='userdb', user='user', password='password')
    
    def enrich_stream_with_reference_data(self, kafka_consumer):
        """ì°¸ì¡° ë°ì´í„°ë¡œ ìŠ¤íŠ¸ë¦¼ ë³´ê°•"""
        # ì°¸ì¡° ë°ì´í„° ìºì‹œ
        reference_cache = {}
        cache_ttl = 300  # 5ë¶„
        
        def get_user_profile(user_id):
            """ì‚¬ìš©ì í”„ë¡œí•„ ì¡°íšŒ (ìºì‹œ ìš°ì„ )"""
            # Redis ìºì‹œ í™•ì¸
            cached_profile = self.redis_client.get(f"user:{user_id}")
            if cached_profile:
                return json.loads(cached_profile)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ
            conn = self.postgres_pool.getconn()
            try:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        SELECT user_id, name, email, tier, credit_limit, 
                               risk_score, created_at
                        FROM users 
                        WHERE user_id = %s
                    """, (user_id,))
                    
                    result = cursor.fetchone()
                    if result:
                        profile = {
                            'user_id': result[0],
                            'name': result[1],
                            'email': result[2],
                            'tier': result[3],
                            'credit_limit': float(result[4]),
                            'risk_score': float(result[5]),
                            'created_at': result[6].isoformat()
                        }
                        
                        # ìºì‹œ ì €ì¥
                        self.redis_client.setex(
                            f"user:{user_id}", 
                            cache_ttl, 
                            json.dumps(profile)
                        )
                        
                        return profile
            finally:
                self.postgres_pool.putconn(conn)
            
            return None
        
        # ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
        for message in kafka_consumer:
            transaction = message.value
            user_id = transaction['user_id']
            
            # ì‚¬ìš©ì í”„ë¡œí•„ë¡œ ë³´ê°•
            user_profile = get_user_profile(user_id)
            
            if user_profile:
                # ë³´ê°•ëœ ì´ë²¤íŠ¸
                enriched_event = {
                    **transaction,
                    'user_profile': user_profile,
                    'risk_assessment': self.assess_risk(
                        transaction, user_profile
                    )
                }
                
                # ë‹¤ìŒ ì²˜ë¦¬ ë‹¨ê³„ë¡œ ì „ì†¡
                yield enriched_event
    
    def assess_risk(self, transaction, user_profile):
        """ì‹¤ì‹œê°„ ë¦¬ìŠ¤í¬ í‰ê°€"""
        risk_factors = []
        risk_score = 0
        
        # ê±°ë˜ ê¸ˆì•¡ vs ì‹ ìš© í•œë„
        if transaction['amount'] > user_profile['credit_limit'] * 0.8:
            risk_factors.append('HIGH_AMOUNT')
            risk_score += 30
        
        # ì‚¬ìš©ì ë¦¬ìŠ¤í¬ ì ìˆ˜
        if user_profile['risk_score'] > 0.7:
            risk_factors.append('HIGH_RISK_USER')
            risk_score += 40
        
        # ì‹œê°„ëŒ€ ë¶„ì„
        hour = datetime.fromisoformat(transaction['timestamp']).hour
        if hour >= 0 and hour <= 6:
            risk_factors.append('UNUSUAL_TIME')
            risk_score += 20
        
        return {
            'risk_score': min(risk_score, 100),
            'risk_factors': risk_factors,
            'requires_review': risk_score > 50
        }
```

## 3. ì‹¤ì‹œê°„ ë¶„ì„ ì—”ì§„

### 3.1 Apache Druid êµ¬í˜„

```python
import requests
import json
from datetime import datetime, timedelta

class DruidRealtimeAnalytics:
    """Apache Druid ì‹¤ì‹œê°„ ë¶„ì„"""
    
    def __init__(self, coordinator_url='http://localhost:8081'):
        self.coordinator_url = coordinator_url
        self.broker_url = 'http://localhost:8082'
        
    def create_streaming_ingestion_spec(self):
        """ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì§‘ ëª…ì„¸ ìƒì„±"""
        ingestion_spec = {
            "type": "kafka",
            "dataSchema": {
                "dataSource": "transactions",
                "parser": {
                    "type": "string",
                    "parseSpec": {
                        "format": "json",
                        "timestampSpec": {
                            "column": "timestamp",
                            "format": "iso"
                        },
                        "dimensionsSpec": {
                            "dimensions": [
                                "transaction_id",
                                "user_id",
                                "category",
                                "payment_method",
                                "merchant_id"
                            ],
                            "dimensionExclusions": [],
                            "spatialDimensions": []
                        }
                    }
                },
                "metricsSpec": [
                    {"type": "count", "name": "count"},
                    {"type": "doubleSum", "name": "total_amount", "fieldName": "amount"},
                    {"type": "doubleMin", "name": "min_amount", "fieldName": "amount"},
                    {"type": "doubleMax", "name": "max_amount", "fieldName": "amount"},
                    {
                        "type": "thetaSketch",
                        "name": "unique_users",
                        "fieldName": "user_id"
                    },
                    {
                        "type": "hyperUnique",
                        "name": "approx_unique_merchants",
                        "fieldName": "merchant_id"
                    }
                ],
                "granularitySpec": {
                    "type": "uniform",
                    "segmentGranularity": "HOUR",
                    "queryGranularity": "MINUTE",
                    "rollup": True
                }
            },
            "tuningConfig": {
                "type": "kafka",
                "reportParseExceptions": True,
                "handoffConditionTimeout": 0,
                "resetOffsetAutomatically": False,
                "segmentWriteOutMediumFactory": {
                    "type": "onHeapMemory"
                },
                "intermediateHandoffPeriod": "PT10M",
                "maxRowsPerSegment": 5000000,
                "maxTotalRows": 20000000
            },
            "ioConfig": {
                "topic": "transactions",
                "consumerProperties": {
                    "bootstrap.servers": "localhost:9092"
                },
                "taskDuration": "PT1H",
                "useEarliestOffset": True,
                "completionTimeout": "PT30M",
                "lateMessageRejectionPeriod": "PT1H"
            }
        }
        
        # ìˆ˜ì§‘ ì‘ì—… ì œì¶œ
        response = requests.post(
            f"{self.coordinator_url}/druid/indexer/v1/supervisor",
            headers={'Content-Type': 'application/json'},
            data=json.dumps(ingestion_spec)
        )
        
        return response.json()
    
    def execute_realtime_queries(self):
        """ì‹¤ì‹œê°„ ì¿¼ë¦¬ ì‹¤í–‰"""
        # TopN ì¿¼ë¦¬ - ìƒìœ„ ì†Œë¹„ì
        topn_query = {
            "queryType": "topN",
            "dataSource": "transactions",
            "intervals": ["2024-01-01/2024-12-31"],
            "granularity": "hour",
            "dimension": "user_id",
            "metric": "total_amount",
            "threshold": 10,
            "aggregations": [
                {"type": "doubleSum", "name": "total_amount", "fieldName": "total_amount"},
                {"type": "count", "name": "transaction_count"}
            ],
            "postAggregations": [
                {
                    "type": "arithmetic",
                    "name": "avg_transaction",
                    "fn": "/",
                    "fields": [
                        {"type": "fieldAccess", "fieldName": "total_amount"},
                        {"type": "fieldAccess", "fieldName": "transaction_count"}
                    ]
                }
            ]
        }
        
        # ì‹œê³„ì—´ ì¿¼ë¦¬
        timeseries_query = {
            "queryType": "timeseries",
            "dataSource": "transactions",
            "intervals": [f"{datetime.now() - timedelta(hours=24)}/{datetime.now()}"],
            "granularity": {
                "type": "period",
                "period": "PT5M"  # 5ë¶„ ê°„ê²©
            },
            "aggregations": [
                {"type": "count", "name": "count"},
                {"type": "doubleSum", "name": "revenue", "fieldName": "total_amount"},
                {"type": "thetaSketch", "name": "unique_users", "fieldName": "unique_users"}
            ],
            "postAggregations": [
                {
                    "type": "thetaSketchEstimate",
                    "name": "unique_user_count",
                    "field": {"type": "fieldAccess", "fieldName": "unique_users"}
                }
            ]
        }
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        response = requests.post(
            f"{self.broker_url}/druid/v2",
            headers={'Content-Type': 'application/json'},
            data=json.dumps(timeseries_query)
        )
        
        return response.json()
```

## 4. ì´ë²¤íŠ¸ ì†Œì‹±ê³¼ CQRS

### 4.1 ì´ë²¤íŠ¸ ì†Œì‹± êµ¬í˜„

```python
import uuid
from abc import ABC, abstractmethod
from typing import List, Dict, Any
import asyncio
import aioredis

class Event(ABC):
    """ì´ë²¤íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    def __init__(self, aggregate_id: str, event_data: Dict[str, Any]):
        self.event_id = str(uuid.uuid4())
        self.aggregate_id = aggregate_id
        self.event_type = self.__class__.__name__
        self.event_data = event_data
        self.timestamp = datetime.utcnow()
        self.version = None

class OrderCreated(Event):
    pass

class OrderItemAdded(Event):
    pass

class OrderConfirmed(Event):
    pass

class OrderShipped(Event):
    pass

class EventStore:
    """ì´ë²¤íŠ¸ ì €ì¥ì†Œ"""
    
    def __init__(self, redis_url='redis://localhost'):
        self.redis_url = redis_url
        self.redis = None
        
    async def connect(self):
        self.redis = await aioredis.create_redis_pool(self.redis_url)
    
    async def append_events(self, aggregate_id: str, events: List[Event], 
                          expected_version: int = -1):
        """ì´ë²¤íŠ¸ ì¶”ê°€"""
        async with self.redis.pipeline() as pipe:
            # ë‚™ê´€ì  ë™ì‹œì„± ì œì–´
            current_version = await self.redis.get(f"version:{aggregate_id}")
            if current_version and int(current_version) != expected_version:
                raise Exception("Concurrency conflict")
            
            for i, event in enumerate(events):
                event.version = expected_version + i + 1
                event_data = {
                    'event_id': event.event_id,
                    'aggregate_id': event.aggregate_id,
                    'event_type': event.event_type,
                    'event_data': json.dumps(event.event_data),
                    'timestamp': event.timestamp.isoformat(),
                    'version': event.version
                }
                
                # ì´ë²¤íŠ¸ ì €ì¥
                pipe.zadd(
                    f"events:{aggregate_id}",
                    {json.dumps(event_data): event.version}
                )
                
                # ê¸€ë¡œë²Œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼
                pipe.xadd(
                    "global-event-stream",
                    {
                        'aggregate_id': aggregate_id,
                        'event_type': event.event_type,
                        'event_data': json.dumps(event_data)
                    }
                )
            
            # ë²„ì „ ì—…ë°ì´íŠ¸
            pipe.set(f"version:{aggregate_id}", expected_version + len(events))
            
            await pipe.execute()
    
    async def get_events(self, aggregate_id: str, from_version: int = 0):
        """ì´ë²¤íŠ¸ ì¡°íšŒ"""
        events_data = await self.redis.zrangebyscore(
            f"events:{aggregate_id}",
            from_version,
            '+inf',
            withscores=True
        )
        
        events = []
        for event_json, version in events_data:
            event_dict = json.loads(event_json)
            events.append(event_dict)
        
        return events

class EventProjector:
    """ì´ë²¤íŠ¸ í”„ë¡œì ì…˜"""
    
    def __init__(self, event_store: EventStore):
        self.event_store = event_store
        self.projections = {}
        
    async def project_order_summary(self):
        """ì£¼ë¬¸ ìš”ì•½ í”„ë¡œì ì…˜"""
        # ê¸€ë¡œë²Œ ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ êµ¬ë…
        last_id = '0'
        
        while True:
            # ìƒˆ ì´ë²¤íŠ¸ ì½ê¸°
            events = await self.event_store.redis.xread(
                ['global-event-stream'],
                latest_ids=[last_id],
                count=100,
                timeout=1000
            )
            
            for stream_name, stream_events in events:
                for event_id, event_data in stream_events:
                    await self.handle_event(event_data)
                    last_id = event_id
    
    async def handle_event(self, event_data):
        """ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        event_type = event_data[b'event_type'].decode()
        aggregate_id = event_data[b'aggregate_id'].decode()
        
        if event_type == 'OrderCreated':
            await self.handle_order_created(aggregate_id, event_data)
        elif event_type == 'OrderItemAdded':
            await self.handle_order_item_added(aggregate_id, event_data)
        # ... ê¸°íƒ€ ì´ë²¤íŠ¸ ì²˜ë¦¬
```

## 5. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì•Œë¦¼

### 5.1 ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê³¼ ì•Œë¦¼

```python
import prometheus_client
from prometheus_client import Counter, Histogram, Gauge
import aiohttp
import asyncio

class RealtimeMonitoringSystem:
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # Prometheus ë©”íŠ¸ë¦­ ì •ì˜
        self.transaction_counter = Counter(
            'transactions_total', 
            'Total number of transactions',
            ['status', 'payment_method']
        )
        
        self.transaction_amount = Histogram(
            'transaction_amount_dollars',
            'Transaction amount in dollars',
            buckets=(10, 50, 100, 500, 1000, 5000, 10000)
        )
        
        self.active_users = Gauge(
            'active_users_current',
            'Currently active users'
        )
        
        self.fraud_detection_latency = Histogram(
            'fraud_detection_latency_seconds',
            'Fraud detection processing time'
        )
        
        # ì•Œë¦¼ ì„ê³„ê°’
        self.alert_thresholds = {
            'high_failure_rate': 0.05,  # 5% ì‹¤íŒ¨ìœ¨
            'high_latency': 1.0,        # 1ì´ˆ
            'low_throughput': 100       # ë¶„ë‹¹ 100ê±´ ë¯¸ë§Œ
        }
        
    async def process_metrics_stream(self, kafka_consumer):
        """ë©”íŠ¸ë¦­ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        window_size = 60  # 1ë¶„ ìœˆë„ìš°
        metrics_window = []
        
        async for message in kafka_consumer:
            transaction = message.value
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            self.transaction_counter.labels(
                status=transaction['status'],
                payment_method=transaction['payment_method']
            ).inc()
            
            self.transaction_amount.observe(transaction['amount'])
            
            # ìœˆë„ìš°ì— ì¶”ê°€
            metrics_window.append({
                'timestamp': datetime.now(),
                'transaction': transaction
            })
            
            # ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
            cutoff_time = datetime.now() - timedelta(seconds=window_size)
            metrics_window = [m for m in metrics_window 
                            if m['timestamp'] > cutoff_time]
            
            # ì‹¤ì‹œê°„ ë¶„ì„
            await self.analyze_window(metrics_window)
    
    async def analyze_window(self, metrics_window):
        """ìœˆë„ìš° ë¶„ì„ ë° ì•Œë¦¼"""
        if not metrics_window:
            return
        
        # ì‹¤íŒ¨ìœ¨ ê³„ì‚°
        total_transactions = len(metrics_window)
        failed_transactions = sum(1 for m in metrics_window 
                                if m['transaction']['status'] == 'FAILED')
        failure_rate = failed_transactions / total_transactions
        
        # ì²˜ë¦¬ëŸ‰ ê³„ì‚° (ë¶„ë‹¹)
        throughput = total_transactions
        
        # ì•Œë¦¼ í™•ì¸
        alerts = []
        
        if failure_rate > self.alert_thresholds['high_failure_rate']:
            alerts.append({
                'type': 'HIGH_FAILURE_RATE',
                'severity': 'critical',
                'value': failure_rate,
                'threshold': self.alert_thresholds['high_failure_rate'],
                'message': f'High failure rate detected: {failure_rate:.2%}'
            })
        
        if throughput < self.alert_thresholds['low_throughput']:
            alerts.append({
                'type': 'LOW_THROUGHPUT',
                'severity': 'warning',
                'value': throughput,
                'threshold': self.alert_thresholds['low_throughput'],
                'message': f'Low throughput detected: {throughput} transactions/min'
            })
        
        # ì•Œë¦¼ ì „ì†¡
        for alert in alerts:
            await self.send_alert(alert)
    
    async def send_alert(self, alert):
        """ì•Œë¦¼ ì „ì†¡"""
        # Slack ì•Œë¦¼
        slack_webhook = "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
        
        async with aiohttp.ClientSession() as session:
            await session.post(slack_webhook, json={
                'text': f"ğŸš¨ {alert['severity'].upper()}: {alert['message']}",
                'attachments': [{
                    'color': 'danger' if alert['severity'] == 'critical' else 'warning',
                    'fields': [
                        {'title': 'Alert Type', 'value': alert['type'], 'short': True},
                        {'title': 'Current Value', 'value': str(alert['value']), 'short': True},
                        {'title': 'Threshold', 'value': str(alert['threshold']), 'short': True},
                        {'title': 'Time', 'value': datetime.now().isoformat(), 'short': True}
                    ]
                }]
            })
```

## ë§ˆë¬´ë¦¬

ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ëŠ” í˜„ëŒ€ ë°ì´í„° ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤. Apache Kafka, Flink, Druidì™€ ê°™ì€ ë„êµ¬ë“¤ì„ í™œìš©í•˜ì—¬ ëŒ€ìš©ëŸ‰ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²¤íŠ¸ ì†Œì‹±ê³¼ CQRS íŒ¨í„´ì€ ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•˜ë©´ì„œë„ ë†’ì€ ì„±ëŠ¥ê³¼ í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹¤ìŒ ì¥ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë‹ˆí„°ë§ê³¼ ì„±ëŠ¥ ì§„ë‹¨ì— ëŒ€í•´ í•™ìŠµí•˜ê² ìŠµë‹ˆë‹¤.